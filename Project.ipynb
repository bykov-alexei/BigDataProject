{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras_metrics import precision, recall\n",
    "from keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import choice\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text, rule):\n",
    "    arr = np.array(list(map(lambda ch: rule[ch], text)))\n",
    "    return arr\n",
    "\n",
    "def get_text(vec, rule):\n",
    "    text = \"\".join(map(lambda i: rule[round(i)], vec))\n",
    "    return text\n",
    "\n",
    "def vec2text(vec):\n",
    "    text = \"\"\n",
    "    for v in vec:\n",
    "        text += vec2char[np.argmax(v)]\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of symbols 58\n",
      "Max length of sonets 242\n"
     ]
    }
   ],
   "source": [
    "with open('data/russian/Esenin.txt', 'r') as f:\n",
    "    text = f.read().lower()\n",
    "poems = text.split(\"\\n\\n\")\n",
    "poem = choice(poems)\n",
    "\n",
    "LATENT_DIM = 50\n",
    "\n",
    "max_len = max(map(len, poems))\n",
    "symbols = set(text) \n",
    "sym_len = len(symbols) + 1\n",
    "print(\"Number of symbols %i\" % sym_len)\n",
    "print(\"Max length of sonets %i\" % max_len)\n",
    "char2vec = {symbol: i + 1 for i, symbol in enumerate(symbols)}\n",
    "char2vec[\"\"] = 0\n",
    "with open('char2vec.pkl', 'wb') as f:\n",
    "    pkl.dump(char2vec, f)\n",
    "vec2char = {i + 1: symbol for i, symbol in enumerate(symbols)}\n",
    "vec2char[0] = \"\"\n",
    "with open('vec2char.pkl', 'wb') as f:\n",
    "    pkl.dump(vec2char, f)\n",
    "    \n",
    "vectorized = np.zeros((len(poems), max_len, sym_len))\n",
    "for i, poem in enumerate(poems):\n",
    "    vectorized[i, range(0, len(poem)), [char2vec[ch] for ch in poem]] = 1\n",
    "    vectorized[i, range(len(poem), max_len), 0] = 1\n",
    "vectorized = vectorized + np.random.normal(0, 1, vectorized.shape) * 0.05\n",
    "np.savez('train.npz', vectorized=vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_1:0' shape=() dtype=int32> fp\n",
      "tracking <tf.Variable 'Variable_2:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_3:0' shape=() dtype=int32> fn\n",
      "WARNING:tensorflow:From /home/alexei/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 242, 58)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 14036)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               1403700   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,807,602\n",
      "Trainable params: 1,403,801\n",
      "Non-trainable params: 1,403,801\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexei/.local/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "inp = layers.Input((max_len, sym_len))\n",
    "x = layers.Flatten()(inp)\n",
    "x = layers.Dense(100, activation='relu')(x)\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "discriminator = models.Model([inp], [x], name='discriminator')\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                        metrics=['acc', precision(), recall()])\n",
    "discriminator.trainable = False\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация данных для предобучения дискриминатора\n",
    "n_wrong = vectorized.shape[0] * 10\n",
    "data = np.random.choice([0, 1], (n_wrong, max_len, sym_len), p=[0.975, 0.025])\n",
    "data = np.concatenate([data, vectorized])\n",
    "y = np.ones(len(data))\n",
    "y[:n_wrong] = 0\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "     data, y, test_size=0.33, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = discriminator.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = layers.Input(shape=(LATENT_DIM,))\n",
    "x = layers.Reshape((LATENT_DIM, 1))(inp)\n",
    "x = layers.Dense(250, activation='relu')(x)\n",
    "x = layers.MaxPool1D(10)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(max_len * sym_len, activation='sigmoid')(x)\n",
    "x = layers.Reshape((max_len, sym_len))(x)\n",
    "\n",
    "generator = models.Model(inp, x, name='generator')\n",
    "generator.compile(optimizer='adam', loss='mse')\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gan\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 50, 1)             0         \n",
      "_________________________________________________________________\n",
      "generator (Model)            (None, 242, 58)           17559536  \n",
      "_________________________________________________________________\n",
      "discriminator (Model)        (None, 1)                 1403801   \n",
      "=================================================================\n",
      "Total params: 18,963,337\n",
      "Trainable params: 17,559,536\n",
      "Non-trainable params: 1,403,801\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan_input = layers.Input(shape=(LATENT_DIM,))\n",
    "x = layers.Reshape((LATENT_DIM, 1))(gan_input)\n",
    "gan_output = discriminator(generator(x))\n",
    "gan = models.Model(gan_input, gan_output, name='gan')\n",
    "\n",
    "discriminator.trainable = False\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 242, 58)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 14036)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                701850    \n",
      "_________________________________________________________________\n",
      "generator (Model)            (None, 242, 58)           17559536  \n",
      "=================================================================\n",
      "Total params: 18,261,386\n",
      "Trainable params: 18,261,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = layers.Input(shape=(max_len, sym_len))\n",
    "x = layers.Flatten()(inp)\n",
    "x = layers.Dense(LATENT_DIM, activation='sigmoid')(x)\n",
    "out = generator(x)\n",
    "\n",
    "encoder = models.Model(inp, out, name=\"encoder\")\n",
    "encoder.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = encoder.fit(vectorized, vectorized, epochs=100, batch_size=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "с коыл н аонте иан то,м\n",
      " ова\n",
      "т нуо а тлвоорйае е моойот ага,сеесннт   всаио оия \n"
     ]
    }
   ],
   "source": [
    "noise = np.random.normal(0, 1,(1,LATENT_DIM,))\n",
    "vec = generator.predict(noise)[0]\n",
    "text = vec2text(vec)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mdiscriminator loss at step 0: -0.047952853\u001b[0m\n",
      "\u001b[32madversarial loss at step 0: 6.9112024\u001b[0m\n",
      "resulting loss at step 0: 6.86325\n"
     ]
    }
   ],
   "source": [
    "iterations = 1\n",
    "batch_size = 20\n",
    "save_dir = 'examples/'\n",
    "\n",
    "losses = {\n",
    "    'adversarial': [],\n",
    "    'discriminator': [],\n",
    "}\n",
    "\n",
    "start = 0\n",
    "step = 0\n",
    "for i in range(iterations):\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, LATENT_DIM))\n",
    "\n",
    "    generated_texts = generator.predict(random_latent_vectors)\n",
    "\n",
    "    stop = start + batch_size\n",
    "    if stop > len(vectorized):\n",
    "        start = 0\n",
    "    stop = start + batch_size\n",
    "    real_texts = vectorized[start: stop]\n",
    "    combined_texts = np.concatenate([generated_texts, real_texts])\n",
    "\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)),\n",
    "                             np.zeros((batch_size, 1))])\n",
    "    labels += 0.05 * np.random.random(labels.shape)\n",
    "\n",
    "    d_loss = discriminator.train_on_batch(combined_texts, labels)[0]\n",
    "\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, LATENT_DIM))\n",
    "\n",
    "    misleading_targets = np.zeros((batch_size, 1))\n",
    "\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    start += batch_size\n",
    "    if start > len(vectorized) - batch_size:\n",
    "        start = 0\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        gan.save_weights('saves/gan-%i.h5' % step)\n",
    "\n",
    "        losses['adversarial'].append(a_loss)\n",
    "        losses['discriminator'].append(d_loss)\n",
    "        \n",
    "        print(f'{Fore.RED}discriminator loss at step %s: %s{Style.RESET_ALL}' % (step, d_loss))\n",
    "        print(f'{Fore.GREEN}adversarial loss at step %s: %s{Style.RESET_ALL}' % (step, a_loss))\n",
    "        print('resulting loss at step %s: %s' % (step, d_loss + a_loss))\n",
    "\n",
    "        text = vec2text(generated_texts[0])\n",
    "        with open(save_dir + 'generated_text-%i.txt' % step, 'w') as f:\n",
    "            f.write(text)\n",
    "        text = vec2text(real_texts[0])\n",
    "        with open(save_dir + 'real_text-%i.txt' % step, 'w') as f:\n",
    "            f.write(text)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
